"""
Check the match in feature bins between the requested feature values and the generated feature values.
The requested feature values are based on the gold source and target sentence pairs. They are written in the
preprocessed src files.
The generated features are the features that the generated text displays and are calculated between the gold source
and the system generation.

The code here:
1) open the original src and the system output and iterate over the sentences
2) extract and remove the features from the src sentence
3) calculate the features between the src and sys sentence
4) with a separate function, inspect these features and check for (mis)matches in bins
5) collect the (mis)matches: a txt file, a line for each sentence pair showing the requested vs generated features
6) print into file analysis of matches and averaged differences for each feature

7) the main function that includes all the above functions will be called in fairseq_preprocess_train_generate.py
"""
import re
import numpy as np
from utils.helpers import yield_lines_in_parallel
from utils.feature_extraction import feature_bins_bundle_sentence
from utils.prepare_word_embeddings_frequency_ranks import load_ranks
from utils.feature_bin_preparation import create_bins

feature2spec_token = {"dependency": "MaxDep", "frequency": "FreqRank", "length": "Length", "levenshtein": "Leven"}
spec_token2feature = {v: k for k, v in feature2spec_token.items()}


def remove_features_from_str(string_prepended_feat):
    # returns the string as well as features (as a dict)
    SPECIAL_TOKEN_REGEX = r'<[a-zA-Z\-_\d\.]+>'
    match = re.match(fr'(^(?:{SPECIAL_TOKEN_REGEX} *)+) *(.*)$', string_prepended_feat)
    if match is None:
        return None, string_prepended_feat
    special_tokens, sentence = match.groups()
    # preprocess the special_tokens string into a dictionary
    feat_dict = {}
    for name_value in special_tokens.split("> <"):
        name_value = name_value.replace(">", "").replace("<", "").split("_")
        name, value = name_value[0], name_value[1]
        feat_dict[name] = float(value)
    return feat_dict, sentence


def analyze_f_match(orig_features, system_features):
    # iterate by keys - feature names
    by_mismatch = {"match": set(), "mismatch": {}}
    for feature, value in orig_features.items():
        if feature not in system_features and spec_token2feature[feature] not in system_features:
            raise KeyError("feature missing in the system output")

        if value == system_features[feature]:
            by_mismatch["match"].add(feature)
        if value != system_features[feature]:
            by_mismatch["mismatch"][feature] = value - system_features[feature]

    return by_mismatch


def parse_file_pair_return_analysis(orig_src_path, system_out_path, requested_features, lang):
    """ orig_src_path is a str full path to the original source file with prepended features
    system_out_path is a str full path to the rewritten text generated by the model
    """
    # load the feature bins and the frequency ranks
    feature_bins = create_bins()

    frequency_ranks = load_ranks(lang)

    feature_matching = {feature2spec_token[f]: {"match": 0, "mismatch": []} for f in requested_features}
    sentence_pair_counter = 0
    # open the files and iterate over lines/sentences
    for src_sent, tgt_sent in yield_lines_in_parallel([orig_src_path, system_out_path], strict=True):
        sentence_pair_counter += 1
        # first remove the prepended feature tokens from the src sentence
        f_vals_bin_orig, src_sent = remove_features_from_str(src_sent)

        # calculate the features between the src and system_out
        f_vals_bin_gen, f_vals_exact = feature_bins_bundle_sentence(src_sent, tgt_sent, lang, requested_features,
                                                                    feature_bins, frequency_ranks)

        # change the feature names to abbreviated versions and round the GEN bins to 2 decimals
        f_vals_bin_gen = {feature2spec_token[f]: round(v, 2) for f, v in f_vals_bin_gen.items()}

        # compare the intended and actual features
        if f_vals_bin_orig:
            sent_feat_match = analyze_f_match(f_vals_bin_orig, f_vals_bin_gen)
            for _f in sent_feat_match["match"]:
                feature_matching[_f]["match"] += 1
            for _misf, _v in sent_feat_match["mismatch"]:
                feature_matching[_misf]["mismatch"].append(_v)

    # print some initial results
    total_matches = sum(f["match"] for f in feature_matching)
    print("Considered features: ", len(requested_features))
    print("Number of sentence pairs %d, number of exact feature bin matches %d" % (sentence_pair_counter, total_matches))
    # TODO plot mismatches ?
    # average divergence from the original feature bin
    for f in feature_matching:
        avg_delta = np.mean(f["mismatch"])
        print("Average divergence from the original bin value for feature %s %f" % (f, avg_delta))

    # TODO save the deltas?




